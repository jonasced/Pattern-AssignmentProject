{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we are going to improve our codes in PattRecClasses and implement forward algorithm inside MarkovChain code as well as functions such as logprob and prob in Guassian in order to generate proper input values for forward algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PattRecClasses import HMM_TA \n",
    "import scipy.stats\n",
    "from matplotlib import pyplot as plt\n",
    "# For the code to work you might have to pip install scipy\n",
    "\n",
    "from CharacterFeatureExtractor import featureExtractor\n",
    "from DrawCharacter import DrawCharacter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Multivariate Gaussian Distribution Class\n",
    "'''\n",
    "class multigaussD:\n",
    "    mean = np.array([0])\n",
    "    cov = np.array([[0]])\n",
    "    def __init__(self, mu, C):\n",
    "        if C.shape[0] is not C.shape[1]:\n",
    "            print(\"error, non-square covariance matrix supplied\")\n",
    "            return\n",
    "        if mu.shape[0] is not C.shape[0]:\n",
    "            print(\"error, mismatched mean vector and covariance matrix dimensions\")\n",
    "            return\n",
    "        self.mean = mu\n",
    "        if np.where(np.diag(C)==0)[0].shape[0] != 0:\n",
    "            C += np.diagflat(np.ones(C.shape[0])/10000)\n",
    "        C[np.isnan(C)]=1\n",
    "        self.cov = C\n",
    "        return\n",
    "    def random(self, num):\n",
    "        return np.random.multivariate_normal(self.mean, self.cov, num)\n",
    "    def rand(self):\n",
    "        return np.random.multivariate_normal(self.mean, self.cov, 1)[0]\n",
    "    def likelihood(self, X):\n",
    "        p = scipy.stats.multivariate_normal(self.mean, self.cov, 1)\n",
    "        pd = p.pdf(X)\n",
    "        return pd\n",
    "    def loghood(self, X):\n",
    "        return np.log(self.likelihood(X))\n",
    "    def getmean(self):\n",
    "        return self.mean\n",
    "    def getcov(self):\n",
    "        return self.cov\n",
    "    \n",
    "def prob(x, B):\n",
    "    T = x.shape[0]\n",
    "    N = B.shape[0]\n",
    "    res = np.zeros((T, N))\n",
    "    for i in range(T):\n",
    "        for j in range(N):\n",
    "            res[i,j] = B[j].likelihood(x[i])\n",
    "    scaled = np.zeros(res.shape)\n",
    "    for i in range(scaled.shape[0]):\n",
    "        for j in range(scaled.shape[1]):\n",
    "            scaled[i, j] = res[i,j]/np.amax(res[i])\n",
    "    return res, scaled\n",
    "\n",
    "\n",
    "def logprob(x, B):\n",
    "    res, scaled = prob(x,B)\n",
    "    return np.log(res), np.log(scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 15)\n",
      "['A', 'C', 'K', 'P', 'X']\n",
      "5\n",
      "(23, 2)\n",
      "<class 'list'>\n",
      "[[ 12.370322   -75.96076991]\n",
      " [ 12.08405109 -65.5530586 ]\n",
      " [ 12.20756073 -55.00499318]\n",
      " [  9.22054958 -49.39571873]\n",
      " [ 12.0425997  -48.36347404]\n",
      " [ 10.81765895 -56.30694585]\n",
      " [ 10.00100512 -53.12711573]\n",
      " [ 10.81765895 -56.30694585]\n",
      " [ 10.00100512 -53.12711573]\n",
      " [ 12.20756073 -55.00499318]\n",
      " [  8.94527703 -63.4319622 ]\n",
      " [  9.22054958 -49.39571873]\n",
      " [ 10.81765895 -56.30694585]\n",
      " [  9.43498625 -57.99163017]\n",
      " [  7.07207293  81.87288427]\n",
      " [ 35.86632721  83.71609326]\n",
      " [ 39.61191323  56.31291909]\n",
      " [ 38.22924053  57.99760341]\n",
      " [ 38.01480386  49.40169198]\n",
      " [ 39.42540521  48.81706145]\n",
      " [ 43.2174645   56.31291909]\n",
      " [ 40.83685398  48.36944728]\n",
      " [ 40.83685398  48.36944728]\n",
      " [ 38.01480386  40.60428127]\n",
      " [ 38.01480386  49.40169198]\n",
      " [ 37.39758467  54.46530883]\n",
      " [ 37.28054077  45.00298662]\n",
      " [ 38.01480386  40.60428127]\n",
      " [ 38.01480386  49.40169198]\n",
      " [ 38.7952594   53.13308897]\n",
      " [ 36.60550908  50.19741553]\n",
      " [ 36.60550908  39.80855771]]\n",
      "Running the Baum Welch Algorithm...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonas/Local/Pattern-Recognition-Project/PattRecClasses/HMM_TA.py:141: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  gammas = np.array(gammas)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-db43bd3f6825>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running the Baum Welch Algorithm...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0mhm_learn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbaum_welch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muselog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Local/Pattern-Recognition-Project/PattRecClasses/HMM_TA.py\u001b[0m in \u001b[0;36mbaum_welch\u001b[0;34m(self, obs, niter, uselog, prin, scaled)\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0malphahats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetahats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalcabc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#from Assignment 3 and 4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mgammas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalcgammas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malphahats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetahats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muselog\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#alpha*beta*c\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m             \u001b[0mnewpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalcinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgammas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muselog\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#average of gammas[:,0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m             \u001b[0mxibar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalcxi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malphahats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetahats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muselog\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#page 132\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0muselog\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Local/Pattern-Recognition-Project/PattRecClasses/HMM_TA.py\u001b[0m in \u001b[0;36mcalcinit\u001b[0;34m(self, gammas, uselog)\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgammas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgammas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgammas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgammas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcalcabc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "### data prep\n",
    "db_name = \"database_test\"\n",
    "data_features = pd.read_pickle(r'data/'+db_name+'_features.cdb')\n",
    "data_labels = pd.read_pickle(r'data/'+db_name+'_labels.cdb')\n",
    "\n",
    "# data_features[k][r] == np.array (ndim, t); K (number of letters) of R samples with Tr individual lengths\n",
    "print((data_features[1][1].shape))\n",
    "print(data_labels)\n",
    "\n",
    "# Works! But test on easier features\n",
    "\n",
    "### train for one character (seen from labels)\n",
    "\n",
    "\n",
    "##### TESTING FOR X, char = 4\n",
    "\n",
    "char = 4\n",
    "obs = data_features[char]\n",
    "\n",
    "# obsTA = np.array([ hm_learn.rand(100)[0] for _ in range(10) ])\n",
    "# print(type(obsTA))\n",
    "# print(obsTA[1].shape) == (100,2)\n",
    "# Our data has format (2,15) ! Transpose all datapoints\n",
    "for i in range(len(obs)):\n",
    "    obs[i] = np.transpose(obs[i])\n",
    "    \n",
    "print(len(obs))\n",
    "print(obs[len(obs)-1].shape)\n",
    "print(type(obs))\n",
    "print(obs[1])\n",
    "\n",
    "\n",
    "### training\n",
    "\n",
    "# Estimate the HMM parameters from the obseved samples\n",
    "# Start by. assigning initial HMM parameter values,\n",
    "# then refine these iteratively\n",
    "\n",
    "### These values are generally true for most of our two-stroke characters\n",
    "qstar = np.array([1, 0])\n",
    "Astar = np.array([[0.9, 0.1], [0, 1]])\n",
    "\n",
    "### Values chosen for X from FeatureVisualization\n",
    "f1s1mean=10\n",
    "f1s2mean=45\n",
    "f2s1mean=-60\n",
    "f2s2mean=70\n",
    "\n",
    "meansstar = np.array( [[f1s1mean, f2s1mean], [f1s2mean, f2s2mean]] )\n",
    "\n",
    "covsstar  = np.array( [[[1, 1],[1, 1]], \n",
    "                       [[1, 1],[1,1]]] )\n",
    "\n",
    "Bstar = np.array([multigaussD(meansstar[0], covsstar[0]),\n",
    "                  multigaussD(meansstar[1], covsstar[1])])\n",
    "\n",
    "\n",
    "hm_learn = HMM_TA.HMM(qstar, Astar, Bstar)\n",
    "\n",
    "print(\"Running the Baum Welch Algorithm...\")\n",
    "hm_learn.baum_welch(obs, 20, prin=1, uselog=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TA test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a HMM\n",
    "q = np.array([0.8, 0.2])\n",
    "A = np.array([[0.95, 0.05],\n",
    "            [0.30, 0.70]])\n",
    "\n",
    "means = np.array( [[0, 0], [2, 2]] )\n",
    "covs  = np.array( [[[1, 2],[2, 4]], \n",
    "                [[1, 0],[0, 3]]] )\n",
    "\n",
    "B = np.array([multigaussD(means[0], covs[0]),\n",
    "            multigaussD(means[1], covs[1])])\n",
    "\n",
    "\n",
    "hm  = HMM_TA.HMM(q, A, B)\n",
    "\n",
    "\n",
    "# Test that baum_welch works with inputs on the same format as us\n",
    "obs = []  \n",
    "for i in range(10): \n",
    "    obs += [hm.rand(100)[0]]\n",
    "\n",
    "print(obs[1].shape)\n",
    "print(obs[1])\n",
    "print(type(obs))\n",
    "\n",
    "print('True HMM parameters:')\n",
    "print('q:')\n",
    "print(q)\n",
    "print('A:')\n",
    "print(A)\n",
    "print('B: means, covariances')\n",
    "print(means)\n",
    "print(covs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the HMM parameters from the obseved samples\n",
    "# Start by. assigning initial HMM parameter values,\n",
    "# then refine these iteratively\n",
    "qstar = np.array([0.8, 0.2])\n",
    "Astar = np.array([[0.5, 0.5], [0.5, 0.5]])\n",
    "\n",
    "meansstar = np.array( [[0, 0], [0, 0]] )\n",
    "\n",
    "covsstar  = np.array( [[[1, 0],[0, 1]], \n",
    "                       [[1, 0],[0,1]]] )\n",
    "\n",
    "Bstar = np.array([multigaussD(meansstar[0], covsstar[0]),\n",
    "                  multigaussD(meansstar[1], covsstar[1])])\n",
    "\n",
    "\n",
    "hm_learn = HMM_TA.HMM(qstar, Astar, Bstar)\n",
    "\n",
    "print(\"Running the Baum Welch Algorithm...\")\n",
    "hm_learn.baum_welch(obs, 20, prin=1, uselog=False)\n",
    "\n",
    "# obs is in the format:\n",
    "# obsTA = np.array([ hm.rand(100)[0] for _ in range(10) ])\n",
    "# ; shape (10 2 100)\n",
    "\n",
    "# test\n",
    "# Test the Viterbi algorithm\n",
    "print(\"Running the Viterbi Algorithm...\")\n",
    "obs, true_states = hm.rand(100)\n",
    "\n",
    "print(\"True States:\\n\",true_states)\n",
    "print(\"Predicted States:\\n\", hm_learn.viterbi(obs))\n",
    "\n",
    "####### SOMETIMES ALL RESULTS ARE TRANSPOSED/INVERTED MATRIXES!!!\n",
    "####### and sometimes it just works. Great stuff\n",
    "\n",
    "# descision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get symbol-1 (small p on top left quadrant)\n",
    "dc1 = np.load(\"data/P_top_left.npy\")\n",
    "\n",
    "#Get symbol-2 (small p on bottom right quadrant)\n",
    "dc2 = np.load(\"data/P_bottom_right.npy\")\n",
    "\n",
    "#Get symbol-3(twice as big p filling entire window)\n",
    "dc3 = np.load(\"data/P_big.npy\")\n",
    "\n",
    "thr = 8 # threshold for sampling and distance normalization\n",
    "\n",
    "# #Feature vectors are returned\n",
    "feature_symbol1, sampled_symbol1 = featureExtractor(dc1,thr,False)\n",
    "feature_symbol2, sampled_symbol2 = featureExtractor(dc2,thr,False)\n",
    "feature_symbol3, sampled_symbol3 = featureExtractor(dc3,thr,False)\n",
    "\n",
    "\n",
    "# normalized distance ,slope, and t for symbol-1\n",
    "f1_symbol1 = feature_symbol1[0]\n",
    "f2_symbol1 = feature_symbol1[1]\n",
    "t1 = np.array(range(0,feature_symbol1.shape[1]))\n",
    "\n",
    "\n",
    "# normalized distance ,slope, and t for symbol-2\n",
    "f1_symbol2 = feature_symbol2[0]\n",
    "f2_symbol2 = feature_symbol2[1]\n",
    "t2 = np.array(range(0,feature_symbol2.shape[1]))\n",
    "\n",
    "# normalized distance ,slope, and t for symbol-2\n",
    "f1_symbol3 = feature_symbol3[0]\n",
    "f2_symbol3 = feature_symbol3[1]\n",
    "t3 = np.array(range(0,feature_symbol3.shape[1]))\n",
    "\n",
    "f, axarr = plt.subplots(3, 3)\n",
    "f.suptitle('Scale & Position Effect', fontsize=20)\n",
    "\n",
    "\n",
    "#------------- SYMBOL DRAWINGS\n",
    "#Drawing of sampled symbol-1\n",
    "axarr[0, 0].scatter(sampled_symbol1[0], sampled_symbol1[1])\n",
    "axarr[0, 0].set(xlabel = \"X-Coordinate\", ylabel = \"Y-Coordinate\")\n",
    "axarr[0, 0].set_title('Symbol-1')\n",
    "axarr[0, 0].set_xlim([0,210])\n",
    "axarr[0, 0].set_ylim([0,210])\n",
    "\n",
    "#Drawing of sampled symbol-2\n",
    "axarr[0, 1].scatter(sampled_symbol2[0], sampled_symbol2[1])\n",
    "axarr[0, 1].set(xlabel = \"X-Coordinate\", ylabel = \"Y-Coordinate\")\n",
    "axarr[0, 1].set_title('Symbol-2')\n",
    "axarr[0, 1].set_xlim([0,210])\n",
    "axarr[0, 1].set_ylim([0,210])\n",
    "\n",
    "#Drawing of sampled symbol-3\n",
    "axarr[0, 2].scatter(sampled_symbol3[0], sampled_symbol3[1])\n",
    "axarr[0, 2].set(xlabel = \"X-Coordinate\", ylabel = \"Y-Coordinate\")\n",
    "axarr[0, 2].set_title('Symbol-3')\n",
    "axarr[0, 2].set_xlim([0,210])\n",
    "axarr[0, 2].set_ylim([0,210])\n",
    "\n",
    "#------------- ABSOLUTE DISTANCE FEATURE\n",
    "\n",
    "#Absolute distance plot of symbol-1\n",
    "axarr[1, 0].plot(t1, f1_symbol1)\n",
    "axarr[1, 0].set(xlabel = \"Time\", ylabel = \"Normalized Distance\")\n",
    "axarr[1, 0].set_ylim([0,np.max(f1_symbol1)])\n",
    "\n",
    "#Absolute distance plot of symbol-2\n",
    "axarr[1, 1].plot(t2, f1_symbol2)\n",
    "axarr[1, 1].set(xlabel = \"Time\", ylabel = \"Normalized Distance\")\n",
    "axarr[1, 1].set_ylim([0,np.max(f1_symbol2)])\n",
    "\n",
    "#Absolute distance plot of symbol-3\n",
    "axarr[1, 2].plot(t3, f1_symbol3)\n",
    "axarr[1, 2].set(xlabel = \"Time\", ylabel = \"Normalized Distance\")\n",
    "axarr[1, 2].set_ylim([0,np.max(f1_symbol3)])\n",
    "\n",
    "#------------- SLOPE FEATURE\n",
    "\n",
    "#Y-wise distance plot of symbol-1\n",
    "axarr[2, 0].plot(t1, f2_symbol1)\n",
    "axarr[2, 0].set(xlabel = \"Time\", ylabel = \"Slope(Degrees)\")\n",
    "axarr[2, 0].set_ylim([-120,120])\n",
    "\n",
    "#Y-wise distance plot of symbol-2\n",
    "axarr[2, 1].plot(t2, f2_symbol2)\n",
    "axarr[2, 1].set(xlabel = \"Time\", ylabel = \"Slope(Degrees)\")\n",
    "axarr[2, 1].set_ylim([-120,120])\n",
    "\n",
    "#Y-wise distance plot of symbol-3\n",
    "axarr[2, 2].plot(t3, f2_symbol3)\n",
    "axarr[2, 2].set(xlabel = \"Time\", ylabel = \"Slope(Degrees)\")\n",
    "axarr[2, 2].set_ylim([-120,120])\n",
    "\n",
    "plt.savefig('fig/P_test.png', bbox_inches='tight', dpi = 300)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
